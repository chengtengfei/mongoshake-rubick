# this is the configuration of mongo-shake.
# if this is your first time using mongo-shake, you can only configue source mongodb address(`mongo_urls`)
# and target mongodb address(`tunnel.address`).
# if you have any problem, please visit https://github.com/alibaba/MongoShake/wiki/FAQ
# 如果有问题，请先查看FAQ文档以及wiki上的说明。

# ----------------------splitter----------------------

# connect source mongodb, set username and password if enable authority.
# split by comma(,) if use multiple instance in one replica-set. E.g., mongodb://username1:password1@primaryA,secondaryB,secondaryC
# split by semicolon(;) if sharding enable. E.g., mongodb://username1:password1@primaryA,secondaryB,secondaryC;mongodb://username2:password2@primaryX,secondaryY,secondaryZ
# 源MongoDB连接串信息，逗号分隔同一个副本集内的结点，分号分隔分片sharding实例，免密模式可以忽略“username:password@”。
# username和password中不能含有特殊字符@和?
# 举例：
# 副本集：mongodb://username1:password1@primaryA,secondaryB,secondaryC
# 分片集：mongodb://username1:password1@primaryA,secondaryB,secondaryC;mongodb://username2:password2@primaryX,secondaryY,secondaryZ
mongo_urls = mongodb://192.168.88.84:27017

# config server of source mongodb sharding
# 源MongoDB config server连接串信息，当源Mongodb是sharding实例时，需要填写
mongo_cs_url =

# connect mode:
# primary: fetch data from primary.
# secondaryPreferred: fetch data from secondary if has, otherwise primary.(default)
# standalone: fetch data from given 1 node, no matter primary, secondary or hidden. This is only
# support when tunnel type is direct.
# 连接模式，primary表示从主上拉取，secondaryPreferred表示优先从secondary拉取（默认建议值），
# standalone表示从任意单个结点拉取。
mongo_connect_mode = secondaryPreferred

# collector name
# id用于输出pid文件等信息。
collector.id = mongoshake

# sync mode: all/document/oplog. default is oplog.
# all means full synchronization + incremental synchronization.
# document means full synchronization.
# oplog means incremental synchronization.
# 同步模式，all表示全量+增量同步，document表示全量同步，oplog表示增量同步。
sync_mode = oplog

# http api interface. Users can use this api to monitor mongoshake.
# We also provide a restful tool named "mongoshake-stat" to
# print ack, lsn, checkpoint and qps information based on this api.
# usage: `./mongoshake-stat --port=9100`
# restful端口，可以查看metric统计情况。
http_profile = 9100
# profiling on net/http/profile
# profiling端口，用于查看内部go堆栈。
system_profile = 9200

# global log level: debug, info, warning, error. lower level message will be filter
log.level = info
# log directory. log and pid file will be stored into this file.
# if not set, default is "./logs/"
# log和pid文件的目录，如果不设置默认打到当前路径的logs目录。
log.dir =
# log file name.
# log文件名。
log.file = collector.log
# log buffer or unbuffer. If set true, logs may not be print when exit. If
# set false, performance will be decreased extremely
# 设置log缓存，true表示包含缓存，如果false那么每条log都会直接刷屏，但对性能有影响；
# 反之，退出不一定能打印所有的log，调试时建议配置false。
log.buffer = false

# filter db or collection namespace. at most one of these two parameters can be given.
# if the filter.namespace.black is not empty, the given namespace will be
# filtered while others namespace passed.
# if the filter.namespace.white is not empty, the given namespace will be
# passed while others filtered.
# all the namespace will be passed if no condition given.
# db and collection connected by the dot(.).
# different namespaces are split by the semicolon(;).
# filter: filterDbName1.filterCollectionName1;filterDbName2
# 黑白名单过滤，目前不支持正则，白名单表示通过的namespace，黑名单表示过滤的namespace，
# 不能同时指定。分号分割不同namespace，每个namespace可以是db，也可以是db.collection。
filter.namespace.black =
filter.namespace.white =
# some databases like "admin", "local", "mongoshake", "config", "system.views" are
# filtered, users can enable these database based on some special needs.
# different database are split by the semicolon(;).
# e.g., admin;mongoshake.
# pay attention: collection isn't support like "admin.xxx" except "system.views"
# 正常情况下，不建议配置该参数，但对于有些非常特殊的场景，用户可以启用admin，mongoshake等库的同步，
# 以分号分割，例如：admin;mongoshake。
filter.pass.special.db =

# this parameter is not supported on current open-source version.
# oplog namespace and global id. others oplog in
# mongo cluster that has distinct global id will
# be discard. Query without gid (means getting all
# oplog out) if no oplog.gid set
# gid用于双活防止环形复制，目前只用于阿里云云上MongoDB，如果是阿里云云上实例互相同步
# 希望开启gid，请联系阿里云售后或者烛昭(vinllen)，sharding的有多个gid请以分号(;)分隔。
oplog.gids =

# [auto]                decide by if there has unique index in collections.
#                               use `collection` if has unique index otherwise use `id`.
# [id]                  shard by ObjectId. handle oplogs in sequence by unique _id
# [collection]  shard by ns. handle oplogs in sequence by unique ns
# hash的方式，id表示按文档hash，collection表示按表hash，auto表示自动选择hash类型。
# 如果没有索引建议选择id达到非常高的同步性能，反之请选择collection。
shard_key = collection

# oplog transmit worker concurrent
# if the source is sharding, worker number must equal to shard numbers.
# 内部发送的worker数目，如果机器性能足够，可以提高worker个数。
worker = 8
# memory queue configuration, plz visit FAQ document to see more details.
# do not modify these variables if the performance and resource usage can
# meet your needs.
# 内部队列的配置参数，如果目前性能足够不建议修改，详细信息参考FAQ。
worker.batch_queue_size = 64
adaptive.batching_max_size = 1024
fetcher.buffer_capacity = 256

# batched oplogs have block level checksum value using
# crc32 algorithm. and compressor for compressing content
# of oplog entry.
# supported compressor are : gzip,zlib,deflate
# Do not enable this option when tunnel type is "direct"
# 是否启用发送，非direct模式发送可以选择压缩以减少网络带宽消耗。
worker.oplog_compressor = none

# tunnel pipeline type. now we support rpc,file,kafka,mock,direct
# 通道模式。
tunnel = kafka
# tunnel target resource url
# for rpc. this is remote receiver socket address
# for tcp. this is remote receiver socket address
# for file. this is the file path, for instance "data"
# for kafka. this is the topic and brokers address which split by comma, for
# instance: topic@brokers1,brokers2, default topic is "mongoshake"
# for mock. this is useless
# for direct. this is target mongodb address which format is the same as `mongo_urls`. If
# the target is sharding, this should be the mongos address.
# direct模式用于直接写入MongoDB，其余模式用于一些分析，或者远距离传输场景，
# 注意，如果是非direct模式，需要通过receiver进行解析，具体参考FAQ文档。
# 此处配置通道的地址，格式与mongo_urls对齐。
tunnel.address = 588743593226215424_pic@192.168.11.57:9092

# collector context storage mainly including store checkpoint.
# checkpoint存储信息，checkpoint本身是一个64位的时间戳表示本次开始拉取的地址。
# type include : database, api
# for api storage, address is http url
# for database storage, address is collection name while db name is "mongoshake" by default.
# checkpoint存储的地址，database表示存储到MongoDB中，api表示提供http的接口写入checkpoint。
context.storage = database
# context.storage.url is only used in to mark the checkpoint store database.
# If the source mongodb type is sharding, the address should be config server when MongoShake's
# version >= 1.5, otherwise, this is replicaSet address.
# When source mongodb type is replicaSet, checkpoint will write into source mongodb as default
# if `context.storage.url` is not set, otherwise, the checkpoint will be written into this
# mongodb. E.g., mongodb://127.0.0.1:20070
# checkpoint的具体写入的MongoDB地址
context.storage.url = mongodb://192.168.88.84:27017
# checkpoint db's name. if context.storage.url is config-server url, need set context.storage.db to admin
# checkpoint存储的数据库名，如果context.storage.url配的是config-server，需要将该参数改为admin
context.storage.db = mongoshake
# the prefix of all checkpoint collection's name.
# checkpoint存储的表名前缀，作为所有checkpoint表的前缀
context.storage.collection = ckpt_default

# real checkpoint: the fetching oplog position
# pay attention: this is UTC time which is 8 hours earlier than CST time. this
# variable will only be used when checkpoint is not exist.
# If checkpoint not exist, and not set context.start_position to 1970-01-01T00:00:00Z,
# then check whether all oplog after context.start_position exist at soure mongodb
# 开始拉取的位置，如果checkpoint已经存在（参考上述存储位置）则该参数无效，
# 如果需要强制该位置开始拉取，需要先删除原来的checkpoint，详见FAQ。
# 若checkpoint不存在，且该值为1970-01-01T00:00:00Z，则会拉取源端所有oplog。
# 若checkpoint不存在，且该值不为1970-01-01T00:00:00Z，则会先检查该时间点之后的oplog是否在源端的节点上存在，
# 如果不是都存在会直接报错退出，如果都存在才会从该位置之后拉取oplog
context.start_position = 1970-01-01T00:00:00Z

# high availability option.
# enable master election if set true. only one mongoshake can become master
# and do sync, the others will wait and at most one of them become master once
# previous master die. The master information stores in the `mongoshake` db in the source
# database by default.
# This option is useless when there is only one mongoshake running.
# 如果开启主备mongoshake拉取同一个源端，此参数需要开启。
master_quorum = false

# transform from source db or collection namespace to dest db or collection namespace.
# at most one of these two parameters can be given.
# transform: fromDbName1.fromCollectionName1:toDbName1.toCollectionName1;fromDbName2:toDbName2
# 转换命名空间，比如a.b同步后变成c.d，谨慎建议开启，比较耗性能。
transform.namespace =
# if use dbref in document rename, need to set it true, but it will decrease performance of replication
# 如果有dbref操作，这个需要置true，谨慎建议开启，比较耗性能。
dbref = false

# if need to enable move chunk at source sharding db, then set it true
# 如果源库是sharding且在增量同步时需要保持开启move chunk，这个需要置true
# 注意即使该参数值为true，在全量同步期间move chunk仍要保持关闭，等到全量同步结束，在做后续增量同步期间可以再手动开启
movechunk.enable = false

# check whether move chunk occurs if movechunk.enable = true.
# 检查movechunk是否产生的时间间隔(ms)
movechunk.interval = 1000

# ----------------------splitter----------------------
# if tunnel type is direct, all the below variable should be set
# 写入MongoDB的参数配置。

# only transfer oplog commands for syncing. represent
# by oplog.op are "i","d","u".
# DDL will be transferred if disable like create index, drop databse,
# transaction in mongodb 4.0.
# 是否需要开启DDL同步，false表示开启，源端支持副本集和sharding
# 如果目的端是sharding，暂时不支持applyOps命令，包括事务。
replayer.dml_only = true

# oplog changes to Insert while Update found non-exist (_id or unique-index)
# 是否将update语句修改为insert语句，如果_id不存在在目的库。
replayer.executor.upsert = false
# oplog changes to Update while Insert found duplicated key (_id or unique-index)
# 是否将insert语句修改为update语句，如果_id存在在目的库。
replayer.executor.insert_on_dup_update = false
# db. write duplicated logs to mongoshake_conflict
# sdk. write duplicated logs to sdk.
# 如果写入存在冲突，记录冲突的文档。
replayer.conflict_write_to = none

# replayer duration mode. drop oplogs and take
# no any action(only for debugging enviroment) if
# set to false. otherwise write to ${mongo_url} instance
# false表示取消写入，只用于调试。
replayer.durable = true

# ----------------------splitter----------------------
# full synchronization configuration
# 全量同步参数配置，详见wiki，如果目的端MongoDB压力过大，可以适当调低下列参数。

# the number of collection concurrence
# 并发最大拉取的表个数。
replayer.collection_parallel = 6

# the number of document concurrence in a collection concurrence
# 同一个表内部并发的线程数。
replayer.document_parallel = 8

# number of documents in a batch insert in a document concurrence
# 目的端写入的batch大小。
replayer.document_batch_size = 256

# drop the same name of collection in dest mongodb in full synchronization
# 如果待同步表在目的端存在，是否先删除目的端的表再进行同步。
# 默认true，表示先删除目的端的表，再同步
# false，表示不删除目的端的表，如果目的端表存在，就不再同步表中的索引和表结构，只同步数据
replayer.collection_drop = true

# filter orphan document to get rid of duplicate id error when source db is sharding
# 若源库是集群实例，全量迁移会自动过滤orphan文档以避免出现duplicate id的报错
filter.orphan_document = false

